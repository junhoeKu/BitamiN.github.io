{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 베이지안 최적화 기법 (25점)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-1 XGBoost , LightGBM 하이퍼 하라미터 튜닝 시에 GridSearch 방식보다는 베이지안 최적화 기법적용하고는 하는데 그 이유는 무엇일까요? (5점)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost와 LGBM은 하이퍼 파라미터 개수가 많은 특징을 가지는데 GridSearch방식은 하이퍼 파라미터의 개수가 많을 경우 최적화 수행 시간이 오래 걸리기 때문에"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-2 아래 코드를 실행하고 문제를 풀어주세요 (20점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset = load_wine()\n",
    "\n",
    "wine = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "wine['target']= dataset.target\n",
    "X_features = wine.iloc[:, :-1]\n",
    "y_label =  wine.iloc[:, -1]\n",
    "\n",
    "# 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출\n",
    "X_train, X_test, y_train, y_test=train_test_split(X_features, y_label, test_size=0.2, random_state=156 )\n",
    "\n",
    "# 앞에서 추출한 학습 데이터를 다시 학습과 검증 데이터로 분리\n",
    "X_tr, X_val, y_tr, y_val= train_test_split(X_train, y_train, test_size=0.1, random_state=156 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) hyperopt 을 이용하여 max_depth는 5에서 15까지 1간격으로, min_child_weight는 1에서 2까지 1간격으로 colsample_bytree는 0.5에서 0.8사이, learning_rate는 0.01에서 0.2사이 정규 분포된 값으로 검색하도록 지정해주세요 (10점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "# max_depth는 5에서 20까지 1간격으로, min_child_weight는 1에서 2까지 1간격으로\n",
    "# colsample_bytree는 0.5에서 1사이, learning_rate는 0.01에서 0.2 사이 정규 분포된 값으로 검색.\n",
    "xgb_search_space = {'max_depth' : hp.quniform('max_depth', 5, 20, 1),\n",
    "                    'min_child_weight' : hp.quniform('min_child_weight', 1, 2, 1),\n",
    "                    'colsample_bytree' : hp.uniform('colsample_bytree', 0.5, 0.8),\n",
    "                    'learning_rate' : hp.uniform('learning_rate', 0.01, 0.2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 아래의 코드를 실행한후 objective_func() 함수와 fmin() 함수를 이용하여 최대 반복 수행횟수는 40회로, rstate는 np.random.default_rng(seed=9)) 로 설정하여 최적의 하이퍼 파라미터를 출력해주세요 (10점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import STATUS_OK\n",
    "\n",
    "# fmin()에서 입력된 search_space 값으로 입력된 모든 값은 실수형임.\n",
    "# XGBClassifier의 정수형 하이퍼 파라미터는 정수형 변환을 해줘야 함.\n",
    "# 정확도는 높을수록 더 좋은 수치임. -1 * 정확도를 곱해서 큰 정확도 값일수록 최소가 되도록 변환\n",
    "def objective_func(search_space):\n",
    "    # 수행 시간 절약을 위해 nestimators는 100으로 축소\n",
    "    xgb_clf = XGBClassifier(n_estimators=100, max_depth=int(search_space['max_depth']),\n",
    "                            min_child_weight=int(search_space['min_child_weight']),\n",
    "                            learning_rate=search_space['learning_rate'],\n",
    "                            colsample_bytree=search_space['colsample_bytree'],\n",
    "                            eval_metric='logloss')\n",
    "    accuracy = cross_val_score(xgb_clf, X_train, y_train, scoring='accuracy', cv=3)\n",
    "\n",
    "    # accuracy는 cv=3 개수만큼 roc-auc 결과를 리스트로 가짐. 이를 평균해서 반환하되 -1을 곱함.\n",
    "    return {'loss':-1 * np.mean(accuracy), 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 40/40 [00:10<00:00,  3.73trial/s, best loss: -0.9859633569739952]\n",
      "best: {'colsample_bytree': 0.500542901278722, 'learning_rate': 0.10327937185006039, 'max_depth': 15.0, 'min_child_weight': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, Trials\n",
    "\n",
    "trial_val = Trials()\n",
    "best = fmin(fn = objective_func,\n",
    "            space = xgb_search_space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 40, # 최대 반복 수행횟수\n",
    "            trials = trial_val, rstate = np.random.default_rng(seed = 9))\n",
    "print('best:', best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 코드를 실행하여 출력해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colsample_bytree:0.50054, learning_rate:0.10328, max_depth:15, min_child_weight:1\n"
     ]
    }
   ],
   "source": [
    "print('colsample_bytree:{0}, learning_rate:{1}, max_depth:{2}, min_child_weight:{3}'.format(\n",
    "    round(best['colsample_bytree'], 5), round(best['learning_rate'], 5),\n",
    "    int(best['max_depth']), int(best['min_child_weight'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 언더 샘플링과 오버 샘플링(20점)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레이블이 불균형한 분포를 가지는 데이터 세트를 학습 시킬 때 예측 성능의 문제가 발생할 수 있습니다. 이때 문에 지도학습에서 극도로 불균형 레이블 값 분포로 인한 문제점을 해결하기 위해서는 적절한 학습 데이터를 확보하는 방안이 필요한데 대표적으로 언더 샘플링과 오버 샘플링 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-1 언더 샘플링과 오버 샘플링의 차이점은 무엇일까요? (10점)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 불균형한 분포를 가질 때 이를 해소할 수 있는 방법이 언더샘플링과 오버샘플링이다.\n",
    " \n",
    "언더샘플링은 불균형한 데이터셋에서 높은 비율을 차지하는 데이터셋의 데이터수를 줄여서 불균형을 해소하는 방법이고\\\n",
    "오버샘플링은 반대로 낮은 비율의 데이터셋의 데이터수를 늘려서 불균형을 해소하는 방법이다.\n",
    "\n",
    "언더샘플링은 학습에 사용되는 전체 데이터 수를 줄이기 때문에 성능이 떨어질 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-2\n",
    "SMOTE 오버 샘플링을 적용하려고 하는 일부 코드를 가져왔다. 아래에서 불필요 없는 코드의 번호를 모두 적으세요. (10점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\82109\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\82109\\anaconda3\\lib\\site-packages (from imblearn) (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.23 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.3.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from scikit-learn>=0.23->imbalanced-learn->imblearn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE 적용 전 학습용 피처/레이블 데이터 세트:  (142, 13) (142,)\n",
      "SMOTE 적용 후 학습용 피처/레이블 데이터 세트:  (171, 13) (171,)\n",
      "SMOTE 적용 후 레이블 값 분포: \n",
      " 1    57\n",
      "0    57\n",
      "2    57\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_over, y_train_over = smote.fit_resample(X_train, y_train)\n",
    "print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_train.shape, y_train.shape)\n",
    "print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_train_over.shape, y_train_over.shape)\n",
    "print('SMOTE 적용 후 레이블 값 분포: \\n', pd.Series(y_train_over).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2, 6, 7, 8 - train 데이터만 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 스태킹 앙상블 (55점)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-1스태킹이 배깅및 부스팅과 공통점과 차이점이 각각 있습니다. 각각 서술해주세요. (10점)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스태킹은 개별 알고리즘을 서로 결합해 예측 결과를 도출한다는 점에서 배깅, 부스팅과 공통점이 있지만\\\n",
    "개별 알고리즘으로 예측한 데이터를 기반으로 다시 예측을 수행한다는 점에 있어서 차이를 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스태킹 모델을 구현해봅시다. 아래코드를 실행해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dataset = load_wine()\n",
    "\n",
    "wine = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "wine['target']= dataset.target\n",
    "X_features = wine.iloc[:, :-1]\n",
    "y_label =  wine.iloc[:, -1]\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(X_features , y_label , test_size=0.2 , random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-2 개별 모델로, knn, 랜덤포레스트, 결정트리, 에이다부스트를 사용할것입니다. 모델을 생성한 후, 학습하고, 예측하여 각각의 정확도를 파악해주세요\n",
    "아래 코드를 사용해주세요 (20점)\n",
    "\n",
    "knn_clf  = KNeighborsClassifier(n_neighbors=4)\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN 정확도: 0.7500\n",
      "RF 정확도: 0.9722\n",
      "DT 정확도: 0.9722\n",
      "ADA 정확도: 0.8333\n"
     ]
    }
   ],
   "source": [
    "# 개별 ML 모델 생성\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=4)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "# 개별 모델들을 학습\n",
    "knn_clf.fit(X_train, y_train)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "# 개별 모델 예측 및 정확도 측정\n",
    "knn_pred = knn_clf.predict(X_test)\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "dt_pred = dt_clf.predict(X_test)\n",
    "ada_pred = ada_clf.predict(X_test)\n",
    "\n",
    "print('KNN 정확도: {0:.4f}'.format(accuracy_score(y_test, knn_pred)))\n",
    "print('RF 정확도: {0:.4f}'.format(accuracy_score(y_test, rf_pred)))\n",
    "print('DT 정확도: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))\n",
    "print('ADA 정확도: {0:.4f}'.format(accuracy_score(y_test, ada_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-3 위의 개별 알고리즘으로부터 예측된 값들을 라벨로 옆으로 붙여서 피처 값으로 만들어주세요. 반환된 예측 데이터 세트는 1차원 형태의 ndarry 이므로 numpy의 transpose()를 이용하면 됩니다. (10점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 36)\n",
      "(36, 4)\n"
     ]
    }
   ],
   "source": [
    "pred = np.array([knn_pred, rf_pred, dt_pred, ada_pred])\n",
    "print(pred.shape)\n",
    "\n",
    "# transpose를 이용해 전치\n",
    "pred = np.transpose(pred)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-4 3-2 의 모델의 예측결과를 합한 데이터 세트로 학습/ 예측하는 최종 모델은 로지스틱 회귀 모델입니다. 최종 메타 모델인 로지스틱회귀를 학습하고 예측 정확도를 측정해주세요. (15점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 메타 모델의 예측 정확도: 1.0000\n"
     ]
    }
   ],
   "source": [
    "lr_final = LogisticRegression()\n",
    "lr_final.fit(pred, y_test)\n",
    "final = lr_final.predict(pred)\n",
    "\n",
    "print('최종 메타 모델의 예측 정확도: {0:.4f}'.format(accuracy_score(y_test, final)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 1. 베이지안 최적화 (25점)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) **다음 빈칸 (A),(B)에 들어갈 단어를 적어주세요** - 1점\n",
    "\n",
    "\n",
    "사이킷런이 제공하는 GridSeachCV API는 분류나 회귀 알고리즘에 사용되는 (A)를 순차적으로 입력하면서 편리하게 최적의 (B)를 도출하는 방안을 제공합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a : 하이퍼 파라미터\n",
    "b : 파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 사이킷런에서 제공하는 Grid Search방식이 XGBoost나 LightGBM 알고리즘에서 하이퍼 파라미터 튜닝에 잘 사용되지 않는 이유에 대해서 적어주세요. - 1점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "조정할 하이퍼 파라미터 수가 너무 많기 때문에 시간이 오래 걸리기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) **다음 빈칸 (C),(D)에 들어갈 단어를 적어주세요** - 1점\n",
    "\n",
    "베이지안 최적화를 구성하는 두 가지 중요 요소에는 (C)와 (D)가 있다. '\n",
    "\n",
    "(C)는 (D)로부터 최적 함수를 예측할 수 있는 값을 추천받은 뒤 이를 기반으로 최적 함수 모델을 개선한다.\n",
    "\n",
    "(D)는 개선된 대체 모델을 기반으로 최적 입력값을 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c : 대체 모델\n",
    "d : 획득 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   베이지안 최적화를 머신러닝 모델의 하이퍼 파라미터 튜닝에 적용할 수 있게 해주는 패키지인 HyperOpt에 대해 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드를 실행해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperopt in c:\\users\\82109\\anaconda3\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: numpy in c:\\users\\82109\\anaconda3\\lib\\site-packages (from hyperopt) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\82109\\anaconda3\\lib\\site-packages (from hyperopt) (1.10.1)\n",
      "Requirement already satisfied: six in c:\\users\\82109\\anaconda3\\lib\\site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from hyperopt) (3.1)\n",
      "Requirement already satisfied: future in c:\\users\\82109\\anaconda3\\lib\\site-packages (from hyperopt) (0.18.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\82109\\anaconda3\\lib\\site-packages (from hyperopt) (4.65.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\82109\\anaconda3\\lib\\site-packages (from hyperopt) (2.2.1)\n",
      "Requirement already satisfied: py4j in c:\\users\\82109\\anaconda3\\lib\\site-packages (from hyperopt) (0.10.9.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\82109\\anaconda3\\lib\\site-packages (from tqdm->hyperopt) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# !pip install hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) **다음 빈칸 (A)에 들어갈 단어를 적어주세요** - 1점\n",
    "\n",
    "HyperOpt는 목적 함수 반환 값의 (A)를 가지는 최적 입력값을 유추한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a : 최솟값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperOpt를 이용한 XGBoost 하이퍼 파라미터 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. 아래 코드를 실행해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from hyperopt import hp, STATUS_OK, fmin, tpe, Trials\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "cancer_df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "cancer_df['target']= dataset.target\n",
    "X_features = cancer_df.iloc[:, :-1]\n",
    "y_label = cancer_df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 추출을 합니다. - 1점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출, random_state=156\n",
    "X_train, X_test, y_train, y_test= train_test_split(X_features, y_label, test_size=0.2, random_state=156 )\n",
    "\n",
    "# 앞에서 추출한 학습 데이터를 다시 학습과 검증 데이터로 분리, random_state=156\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=156 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 입력값의 검색공간을 설정해줍니다.(hyperOpt의 함수 사용) - 5점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth는 4에서 16까지 1간격으로, min_child_weight는 1에서 3까지 1간격으로\n",
    "# colsample_bytree는 0.4에서 0.8사이, learning_rate는 0.005에서 0.2 사이 정규 분포된 값으로 검색.\n",
    "\n",
    "xgb_search_space = {'max_depth' : hp.quniform('max_depth', 4, 16, 1),\n",
    "                    'min_child_weight' : hp.quniform('min_child_weight', 1, 3, 1),\n",
    "                    'colsample_bytree' : hp.uniform('colsample_bytree', 0.4, 0.8),\n",
    "                    'learning_rate' : hp.uniform('learning_rate', 0.005, 0.2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 입력 값에 따른 목적 함수를 설정해줍니다. - 5점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 공간에서 목적 함수로 입력되는 모든 인자들은 실수형 값이므로 정수형 변환을 해줘야 함\n",
    "# 정확도는 높을수록 더 좋은 수치인데 최솟값을 가지는 입력값을 유추하니 어떻게 변환하면 좋을까요?\n",
    "# accuracy는 cross_val_score로 판단(scoring=\"accuracy\"), cv=3\n",
    "\n",
    "def objective_func(search_space):\n",
    "\n",
    "    xgb_clf = XGBClassifier(n_estimators=400, max_depth = int(search_space['max_depth']),\n",
    "                            min_child_weight = int(search_space['min_child_weight']),\n",
    "                            learning_rate = search_space['learning_rate'],\n",
    "                            colsample_bytree = search_space['colsample_bytree'],\n",
    "                            eval_metric='logloss')\n",
    "    accuracy = cross_val_score(xgb_clf, X_train, y_train, scoring='accuracy', cv=3)\n",
    "    \n",
    "    return {'loss':-1 * np.mean(accuracy), 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. fmin()을 이용해 최적 하이퍼 파라미터를 도출해줍니다. - 5점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 40/40 [00:22<00:00,  1.80trial/s, best loss: -0.9692546764261647]\n",
      "{'colsample_bytree': 0.6213906574722501, 'learning_rate': 0.08671934701542167, 'max_depth': 5.0, 'min_child_weight': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# 최대 반복 횟수은 40\n",
    "\n",
    "trial_val = Trials()\n",
    "best = fmin(fn=objective_func,\n",
    "            space=xgb_search_space,\n",
    "            max_evals=40, \n",
    "            algo=tpe.suggest,\n",
    "            trials=trial_val, rstate=np.random.default_rng(seed=9))\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 도출된 최적 하이퍼 파라미터들을 이용해 XGBClassifier를 재학습한 후 성능을 평가해봅니다. - 5점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.62034\tvalidation_1-logloss:0.64186\n",
      "[1]\tvalidation_0-logloss:0.55891\tvalidation_1-logloss:0.59504\n",
      "[2]\tvalidation_0-logloss:0.50503\tvalidation_1-logloss:0.55529\n",
      "[3]\tvalidation_0-logloss:0.45905\tvalidation_1-logloss:0.52119\n",
      "[4]\tvalidation_0-logloss:0.41931\tvalidation_1-logloss:0.49871\n",
      "[5]\tvalidation_0-logloss:0.38439\tvalidation_1-logloss:0.47909\n",
      "[6]\tvalidation_0-logloss:0.35320\tvalidation_1-logloss:0.45256\n",
      "[7]\tvalidation_0-logloss:0.32550\tvalidation_1-logloss:0.43076\n",
      "[8]\tvalidation_0-logloss:0.30033\tvalidation_1-logloss:0.41120\n",
      "[9]\tvalidation_0-logloss:0.27697\tvalidation_1-logloss:0.39340\n",
      "[10]\tvalidation_0-logloss:0.25611\tvalidation_1-logloss:0.37844\n",
      "[11]\tvalidation_0-logloss:0.23801\tvalidation_1-logloss:0.36968\n",
      "[12]\tvalidation_0-logloss:0.22207\tvalidation_1-logloss:0.35956\n",
      "[13]\tvalidation_0-logloss:0.20753\tvalidation_1-logloss:0.34794\n",
      "[14]\tvalidation_0-logloss:0.19357\tvalidation_1-logloss:0.33872\n",
      "[15]\tvalidation_0-logloss:0.18123\tvalidation_1-logloss:0.33185\n",
      "[16]\tvalidation_0-logloss:0.17048\tvalidation_1-logloss:0.32332\n",
      "[17]\tvalidation_0-logloss:0.15990\tvalidation_1-logloss:0.31774\n",
      "[18]\tvalidation_0-logloss:0.14933\tvalidation_1-logloss:0.30979\n",
      "[19]\tvalidation_0-logloss:0.14057\tvalidation_1-logloss:0.30517\n",
      "[20]\tvalidation_0-logloss:0.13265\tvalidation_1-logloss:0.30151\n",
      "[21]\tvalidation_0-logloss:0.12523\tvalidation_1-logloss:0.29777\n",
      "[22]\tvalidation_0-logloss:0.11809\tvalidation_1-logloss:0.29366\n",
      "[23]\tvalidation_0-logloss:0.11243\tvalidation_1-logloss:0.29027\n",
      "[24]\tvalidation_0-logloss:0.10708\tvalidation_1-logloss:0.28449\n",
      "[25]\tvalidation_0-logloss:0.10219\tvalidation_1-logloss:0.28107\n",
      "[26]\tvalidation_0-logloss:0.09746\tvalidation_1-logloss:0.28001\n",
      "[27]\tvalidation_0-logloss:0.09222\tvalidation_1-logloss:0.27444\n",
      "[28]\tvalidation_0-logloss:0.08719\tvalidation_1-logloss:0.26984\n",
      "[29]\tvalidation_0-logloss:0.08308\tvalidation_1-logloss:0.26881\n",
      "[30]\tvalidation_0-logloss:0.07942\tvalidation_1-logloss:0.26723\n",
      "[31]\tvalidation_0-logloss:0.07644\tvalidation_1-logloss:0.26890\n",
      "[32]\tvalidation_0-logloss:0.07354\tvalidation_1-logloss:0.26761\n",
      "[33]\tvalidation_0-logloss:0.07099\tvalidation_1-logloss:0.26650\n",
      "[34]\tvalidation_0-logloss:0.06844\tvalidation_1-logloss:0.26725\n",
      "[35]\tvalidation_0-logloss:0.06593\tvalidation_1-logloss:0.26871\n",
      "[36]\tvalidation_0-logloss:0.06291\tvalidation_1-logloss:0.26745\n",
      "[37]\tvalidation_0-logloss:0.06033\tvalidation_1-logloss:0.26705\n",
      "[38]\tvalidation_0-logloss:0.05844\tvalidation_1-logloss:0.26885\n",
      "[39]\tvalidation_0-logloss:0.05612\tvalidation_1-logloss:0.26781\n",
      "[40]\tvalidation_0-logloss:0.05452\tvalidation_1-logloss:0.26595\n",
      "[41]\tvalidation_0-logloss:0.05252\tvalidation_1-logloss:0.26350\n",
      "[42]\tvalidation_0-logloss:0.05069\tvalidation_1-logloss:0.26311\n",
      "[43]\tvalidation_0-logloss:0.04932\tvalidation_1-logloss:0.26322\n",
      "[44]\tvalidation_0-logloss:0.04763\tvalidation_1-logloss:0.26551\n",
      "[45]\tvalidation_0-logloss:0.04600\tvalidation_1-logloss:0.26317\n",
      "[46]\tvalidation_0-logloss:0.04462\tvalidation_1-logloss:0.26414\n",
      "[47]\tvalidation_0-logloss:0.04350\tvalidation_1-logloss:0.26402\n",
      "[48]\tvalidation_0-logloss:0.04245\tvalidation_1-logloss:0.26512\n",
      "[49]\tvalidation_0-logloss:0.04137\tvalidation_1-logloss:0.26437\n",
      "[50]\tvalidation_0-logloss:0.04013\tvalidation_1-logloss:0.26251\n",
      "[51]\tvalidation_0-logloss:0.03918\tvalidation_1-logloss:0.26340\n",
      "[52]\tvalidation_0-logloss:0.03812\tvalidation_1-logloss:0.26428\n",
      "[53]\tvalidation_0-logloss:0.03727\tvalidation_1-logloss:0.26489\n",
      "[54]\tvalidation_0-logloss:0.03639\tvalidation_1-logloss:0.26477\n",
      "[55]\tvalidation_0-logloss:0.03570\tvalidation_1-logloss:0.26473\n",
      "[56]\tvalidation_0-logloss:0.03471\tvalidation_1-logloss:0.26267\n",
      "[57]\tvalidation_0-logloss:0.03388\tvalidation_1-logloss:0.26489\n",
      "[58]\tvalidation_0-logloss:0.03334\tvalidation_1-logloss:0.26676\n",
      "[59]\tvalidation_0-logloss:0.03244\tvalidation_1-logloss:0.26807\n",
      "[60]\tvalidation_0-logloss:0.03163\tvalidation_1-logloss:0.26911\n",
      "[61]\tvalidation_0-logloss:0.03094\tvalidation_1-logloss:0.26833\n",
      "[62]\tvalidation_0-logloss:0.03043\tvalidation_1-logloss:0.26878\n",
      "[63]\tvalidation_0-logloss:0.02982\tvalidation_1-logloss:0.26807\n",
      "[64]\tvalidation_0-logloss:0.02947\tvalidation_1-logloss:0.27007\n",
      "[65]\tvalidation_0-logloss:0.02904\tvalidation_1-logloss:0.26769\n",
      "[66]\tvalidation_0-logloss:0.02861\tvalidation_1-logloss:0.26815\n",
      "[67]\tvalidation_0-logloss:0.02819\tvalidation_1-logloss:0.26941\n",
      "[68]\tvalidation_0-logloss:0.02781\tvalidation_1-logloss:0.26990\n",
      "[69]\tvalidation_0-logloss:0.02744\tvalidation_1-logloss:0.26847\n",
      "[70]\tvalidation_0-logloss:0.02712\tvalidation_1-logloss:0.26530\n",
      "[71]\tvalidation_0-logloss:0.02678\tvalidation_1-logloss:0.26396\n",
      "[72]\tvalidation_0-logloss:0.02644\tvalidation_1-logloss:0.26445\n",
      "[73]\tvalidation_0-logloss:0.02612\tvalidation_1-logloss:0.26321\n",
      "[74]\tvalidation_0-logloss:0.02582\tvalidation_1-logloss:0.26200\n",
      "[75]\tvalidation_0-logloss:0.02560\tvalidation_1-logloss:0.26101\n",
      "[76]\tvalidation_0-logloss:0.02529\tvalidation_1-logloss:0.26152\n",
      "[77]\tvalidation_0-logloss:0.02498\tvalidation_1-logloss:0.26228\n",
      "[78]\tvalidation_0-logloss:0.02468\tvalidation_1-logloss:0.26269\n",
      "[79]\tvalidation_0-logloss:0.02441\tvalidation_1-logloss:0.25979\n",
      "[80]\tvalidation_0-logloss:0.02408\tvalidation_1-logloss:0.25921\n",
      "[81]\tvalidation_0-logloss:0.02385\tvalidation_1-logloss:0.25810\n",
      "[82]\tvalidation_0-logloss:0.02363\tvalidation_1-logloss:0.25546\n",
      "[83]\tvalidation_0-logloss:0.02338\tvalidation_1-logloss:0.25446\n",
      "[84]\tvalidation_0-logloss:0.02310\tvalidation_1-logloss:0.25526\n",
      "[85]\tvalidation_0-logloss:0.02284\tvalidation_1-logloss:0.25605\n",
      "[86]\tvalidation_0-logloss:0.02261\tvalidation_1-logloss:0.25505\n",
      "[87]\tvalidation_0-logloss:0.02238\tvalidation_1-logloss:0.25554\n",
      "[88]\tvalidation_0-logloss:0.02206\tvalidation_1-logloss:0.25343\n",
      "[89]\tvalidation_0-logloss:0.02184\tvalidation_1-logloss:0.25381\n",
      "[90]\tvalidation_0-logloss:0.02156\tvalidation_1-logloss:0.25196\n",
      "[91]\tvalidation_0-logloss:0.02134\tvalidation_1-logloss:0.25279\n",
      "[92]\tvalidation_0-logloss:0.02118\tvalidation_1-logloss:0.25185\n",
      "[93]\tvalidation_0-logloss:0.02108\tvalidation_1-logloss:0.25164\n",
      "[94]\tvalidation_0-logloss:0.02098\tvalidation_1-logloss:0.25288\n",
      "[95]\tvalidation_0-logloss:0.02083\tvalidation_1-logloss:0.25266\n",
      "[96]\tvalidation_0-logloss:0.02074\tvalidation_1-logloss:0.25183\n",
      "[97]\tvalidation_0-logloss:0.02065\tvalidation_1-logloss:0.25165\n",
      "[98]\tvalidation_0-logloss:0.02056\tvalidation_1-logloss:0.25288\n",
      "[99]\tvalidation_0-logloss:0.02042\tvalidation_1-logloss:0.25270\n",
      "[100]\tvalidation_0-logloss:0.02034\tvalidation_1-logloss:0.25389\n",
      "[101]\tvalidation_0-logloss:0.02021\tvalidation_1-logloss:0.25469\n",
      "[102]\tvalidation_0-logloss:0.02014\tvalidation_1-logloss:0.25582\n",
      "[103]\tvalidation_0-logloss:0.02005\tvalidation_1-logloss:0.25497\n",
      "[104]\tvalidation_0-logloss:0.01998\tvalidation_1-logloss:0.25489\n",
      "[105]\tvalidation_0-logloss:0.01990\tvalidation_1-logloss:0.25411\n",
      "[106]\tvalidation_0-logloss:0.01983\tvalidation_1-logloss:0.25394\n",
      "[107]\tvalidation_0-logloss:0.01976\tvalidation_1-logloss:0.25389\n",
      "[108]\tvalidation_0-logloss:0.01968\tvalidation_1-logloss:0.25505\n",
      "[109]\tvalidation_0-logloss:0.01961\tvalidation_1-logloss:0.25614\n",
      "[110]\tvalidation_0-logloss:0.01955\tvalidation_1-logloss:0.25600\n",
      "[111]\tvalidation_0-logloss:0.01947\tvalidation_1-logloss:0.25522\n",
      "[112]\tvalidation_0-logloss:0.01940\tvalidation_1-logloss:0.25449\n",
      "[113]\tvalidation_0-logloss:0.01934\tvalidation_1-logloss:0.25436\n",
      "[114]\tvalidation_0-logloss:0.01928\tvalidation_1-logloss:0.25553\n",
      "[115]\tvalidation_0-logloss:0.01922\tvalidation_1-logloss:0.25549\n",
      "[116]\tvalidation_0-logloss:0.01915\tvalidation_1-logloss:0.25660\n",
      "[117]\tvalidation_0-logloss:0.01909\tvalidation_1-logloss:0.25590\n",
      "[118]\tvalidation_0-logloss:0.01903\tvalidation_1-logloss:0.25465\n",
      "[119]\tvalidation_0-logloss:0.01897\tvalidation_1-logloss:0.25456\n",
      "[120]\tvalidation_0-logloss:0.01891\tvalidation_1-logloss:0.25566\n",
      "[121]\tvalidation_0-logloss:0.01885\tvalidation_1-logloss:0.25500\n",
      "[122]\tvalidation_0-logloss:0.01879\tvalidation_1-logloss:0.25492\n",
      "[123]\tvalidation_0-logloss:0.01874\tvalidation_1-logloss:0.25349\n",
      "[124]\tvalidation_0-logloss:0.01867\tvalidation_1-logloss:0.25458\n",
      "[125]\tvalidation_0-logloss:0.01861\tvalidation_1-logloss:0.25491\n",
      "[126]\tvalidation_0-logloss:0.01856\tvalidation_1-logloss:0.25594\n",
      "[127]\tvalidation_0-logloss:0.01850\tvalidation_1-logloss:0.25529\n",
      "[128]\tvalidation_0-logloss:0.01845\tvalidation_1-logloss:0.25517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[129]\tvalidation_0-logloss:0.01840\tvalidation_1-logloss:0.25377\n",
      "[130]\tvalidation_0-logloss:0.01834\tvalidation_1-logloss:0.25369\n",
      "[131]\tvalidation_0-logloss:0.01829\tvalidation_1-logloss:0.25473\n",
      "[132]\tvalidation_0-logloss:0.01824\tvalidation_1-logloss:0.25412\n",
      "[133]\tvalidation_0-logloss:0.01818\tvalidation_1-logloss:0.25445\n",
      "[134]\tvalidation_0-logloss:0.01813\tvalidation_1-logloss:0.25368\n",
      "[135]\tvalidation_0-logloss:0.01808\tvalidation_1-logloss:0.25400\n",
      "[136]\tvalidation_0-logloss:0.01803\tvalidation_1-logloss:0.25262\n",
      "[137]\tvalidation_0-logloss:0.01798\tvalidation_1-logloss:0.25362\n",
      "[138]\tvalidation_0-logloss:0.01793\tvalidation_1-logloss:0.25355\n",
      "[139]\tvalidation_0-logloss:0.01788\tvalidation_1-logloss:0.25411\n",
      "[140]\tvalidation_0-logloss:0.01783\tvalidation_1-logloss:0.25273\n",
      "[141]\tvalidation_0-logloss:0.01778\tvalidation_1-logloss:0.25154\n",
      "[142]\tvalidation_0-logloss:0.01773\tvalidation_1-logloss:0.25253\n",
      "[143]\tvalidation_0-logloss:0.01768\tvalidation_1-logloss:0.25240\n",
      "[144]\tvalidation_0-logloss:0.01764\tvalidation_1-logloss:0.25296\n",
      "[145]\tvalidation_0-logloss:0.01759\tvalidation_1-logloss:0.25234\n",
      "[146]\tvalidation_0-logloss:0.01754\tvalidation_1-logloss:0.25161\n",
      "[147]\tvalidation_0-logloss:0.01750\tvalidation_1-logloss:0.25148\n",
      "[148]\tvalidation_0-logloss:0.01745\tvalidation_1-logloss:0.25180\n",
      "[149]\tvalidation_0-logloss:0.01740\tvalidation_1-logloss:0.25047\n",
      "[150]\tvalidation_0-logloss:0.01736\tvalidation_1-logloss:0.25040\n",
      "[151]\tvalidation_0-logloss:0.01732\tvalidation_1-logloss:0.24969\n",
      "[152]\tvalidation_0-logloss:0.01727\tvalidation_1-logloss:0.25024\n",
      "[153]\tvalidation_0-logloss:0.01723\tvalidation_1-logloss:0.24897\n",
      "[154]\tvalidation_0-logloss:0.01719\tvalidation_1-logloss:0.24892\n",
      "[155]\tvalidation_0-logloss:0.01714\tvalidation_1-logloss:0.24948\n",
      "[156]\tvalidation_0-logloss:0.01710\tvalidation_1-logloss:0.24892\n",
      "[157]\tvalidation_0-logloss:0.01706\tvalidation_1-logloss:0.24886\n",
      "[158]\tvalidation_0-logloss:0.01702\tvalidation_1-logloss:0.24819\n",
      "[159]\tvalidation_0-logloss:0.01698\tvalidation_1-logloss:0.24872\n",
      "[160]\tvalidation_0-logloss:0.01694\tvalidation_1-logloss:0.24749\n",
      "[161]\tvalidation_0-logloss:0.01690\tvalidation_1-logloss:0.24848\n",
      "[162]\tvalidation_0-logloss:0.01686\tvalidation_1-logloss:0.24843\n",
      "[163]\tvalidation_0-logloss:0.01682\tvalidation_1-logloss:0.24885\n",
      "[164]\tvalidation_0-logloss:0.01679\tvalidation_1-logloss:0.24847\n",
      "[165]\tvalidation_0-logloss:0.01675\tvalidation_1-logloss:0.24834\n",
      "[166]\tvalidation_0-logloss:0.01671\tvalidation_1-logloss:0.24887\n",
      "[167]\tvalidation_0-logloss:0.01667\tvalidation_1-logloss:0.24821\n",
      "[168]\tvalidation_0-logloss:0.01664\tvalidation_1-logloss:0.24823\n",
      "[169]\tvalidation_0-logloss:0.01660\tvalidation_1-logloss:0.24769\n",
      "[170]\tvalidation_0-logloss:0.01656\tvalidation_1-logloss:0.24648\n",
      "[171]\tvalidation_0-logloss:0.01652\tvalidation_1-logloss:0.24643\n",
      "[172]\tvalidation_0-logloss:0.01648\tvalidation_1-logloss:0.24529\n",
      "[173]\tvalidation_0-logloss:0.01645\tvalidation_1-logloss:0.24581\n",
      "[174]\tvalidation_0-logloss:0.01641\tvalidation_1-logloss:0.24622\n",
      "[175]\tvalidation_0-logloss:0.01638\tvalidation_1-logloss:0.24609\n",
      "[176]\tvalidation_0-logloss:0.01634\tvalidation_1-logloss:0.24572\n",
      "[177]\tvalidation_0-logloss:0.01631\tvalidation_1-logloss:0.24460\n",
      "[178]\tvalidation_0-logloss:0.01627\tvalidation_1-logloss:0.24463\n",
      "[179]\tvalidation_0-logloss:0.01623\tvalidation_1-logloss:0.24559\n",
      "[180]\tvalidation_0-logloss:0.01620\tvalidation_1-logloss:0.24450\n",
      "[181]\tvalidation_0-logloss:0.01616\tvalidation_1-logloss:0.24445\n",
      "[182]\tvalidation_0-logloss:0.01613\tvalidation_1-logloss:0.24449\n",
      "[183]\tvalidation_0-logloss:0.01610\tvalidation_1-logloss:0.24543\n",
      "[184]\tvalidation_0-logloss:0.01606\tvalidation_1-logloss:0.24494\n",
      "[185]\tvalidation_0-logloss:0.01603\tvalidation_1-logloss:0.24490\n",
      "[186]\tvalidation_0-logloss:0.01600\tvalidation_1-logloss:0.24384\n",
      "[187]\tvalidation_0-logloss:0.01597\tvalidation_1-logloss:0.24428\n",
      "[188]\tvalidation_0-logloss:0.01594\tvalidation_1-logloss:0.24379\n",
      "[189]\tvalidation_0-logloss:0.01590\tvalidation_1-logloss:0.24383\n",
      "[190]\tvalidation_0-logloss:0.01587\tvalidation_1-logloss:0.24347\n",
      "[191]\tvalidation_0-logloss:0.01584\tvalidation_1-logloss:0.24334\n",
      "[192]\tvalidation_0-logloss:0.01581\tvalidation_1-logloss:0.24426\n",
      "[193]\tvalidation_0-logloss:0.01578\tvalidation_1-logloss:0.24323\n",
      "[194]\tvalidation_0-logloss:0.01574\tvalidation_1-logloss:0.24327\n",
      "[195]\tvalidation_0-logloss:0.01572\tvalidation_1-logloss:0.24369\n",
      "[196]\tvalidation_0-logloss:0.01568\tvalidation_1-logloss:0.24366\n",
      "[197]\tvalidation_0-logloss:0.01565\tvalidation_1-logloss:0.24320\n",
      "[198]\tvalidation_0-logloss:0.01562\tvalidation_1-logloss:0.24324\n",
      "[199]\tvalidation_0-logloss:0.01559\tvalidation_1-logloss:0.24222\n",
      "[200]\tvalidation_0-logloss:0.01556\tvalidation_1-logloss:0.24219\n",
      "[201]\tvalidation_0-logloss:0.01554\tvalidation_1-logloss:0.24262\n",
      "[202]\tvalidation_0-logloss:0.01551\tvalidation_1-logloss:0.24227\n",
      "[203]\tvalidation_0-logloss:0.01548\tvalidation_1-logloss:0.24214\n",
      "[204]\tvalidation_0-logloss:0.01545\tvalidation_1-logloss:0.24167\n",
      "[205]\tvalidation_0-logloss:0.01542\tvalidation_1-logloss:0.24257\n",
      "[206]\tvalidation_0-logloss:0.01539\tvalidation_1-logloss:0.24213\n",
      "[207]\tvalidation_0-logloss:0.01536\tvalidation_1-logloss:0.24255\n",
      "[208]\tvalidation_0-logloss:0.01533\tvalidation_1-logloss:0.24261\n",
      "[209]\tvalidation_0-logloss:0.01531\tvalidation_1-logloss:0.24257\n",
      "[210]\tvalidation_0-logloss:0.01528\tvalidation_1-logloss:0.24156\n",
      "[211]\tvalidation_0-logloss:0.01525\tvalidation_1-logloss:0.24243\n",
      "[212]\tvalidation_0-logloss:0.01522\tvalidation_1-logloss:0.24259\n",
      "[213]\tvalidation_0-logloss:0.01519\tvalidation_1-logloss:0.24226\n",
      "[214]\tvalidation_0-logloss:0.01517\tvalidation_1-logloss:0.24231\n",
      "[215]\tvalidation_0-logloss:0.01514\tvalidation_1-logloss:0.24133\n",
      "[216]\tvalidation_0-logloss:0.01511\tvalidation_1-logloss:0.24219\n",
      "[217]\tvalidation_0-logloss:0.01509\tvalidation_1-logloss:0.24205\n",
      "[218]\tvalidation_0-logloss:0.01506\tvalidation_1-logloss:0.24162\n",
      "[219]\tvalidation_0-logloss:0.01503\tvalidation_1-logloss:0.24168\n",
      "[220]\tvalidation_0-logloss:0.01501\tvalidation_1-logloss:0.24074\n",
      "[221]\tvalidation_0-logloss:0.01498\tvalidation_1-logloss:0.24088\n",
      "[222]\tvalidation_0-logloss:0.01496\tvalidation_1-logloss:0.24075\n",
      "[223]\tvalidation_0-logloss:0.01493\tvalidation_1-logloss:0.24118\n",
      "[224]\tvalidation_0-logloss:0.01491\tvalidation_1-logloss:0.24076\n",
      "[225]\tvalidation_0-logloss:0.01488\tvalidation_1-logloss:0.24118\n",
      "[226]\tvalidation_0-logloss:0.01486\tvalidation_1-logloss:0.24114\n",
      "[227]\tvalidation_0-logloss:0.01483\tvalidation_1-logloss:0.24081\n",
      "[228]\tvalidation_0-logloss:0.01481\tvalidation_1-logloss:0.24039\n",
      "[229]\tvalidation_0-logloss:0.01478\tvalidation_1-logloss:0.24122\n",
      "[230]\tvalidation_0-logloss:0.01476\tvalidation_1-logloss:0.24129\n",
      "[231]\tvalidation_0-logloss:0.01473\tvalidation_1-logloss:0.24144\n",
      "[232]\tvalidation_0-logloss:0.01471\tvalidation_1-logloss:0.24051\n",
      "[233]\tvalidation_0-logloss:0.01468\tvalidation_1-logloss:0.24038\n",
      "[234]\tvalidation_0-logloss:0.01466\tvalidation_1-logloss:0.24119\n",
      "[235]\tvalidation_0-logloss:0.01464\tvalidation_1-logloss:0.24134\n",
      "[236]\tvalidation_0-logloss:0.01461\tvalidation_1-logloss:0.24095\n",
      "[237]\tvalidation_0-logloss:0.01459\tvalidation_1-logloss:0.24055\n",
      "[238]\tvalidation_0-logloss:0.01457\tvalidation_1-logloss:0.24070\n",
      "[239]\tvalidation_0-logloss:0.01455\tvalidation_1-logloss:0.24041\n",
      "[240]\tvalidation_0-logloss:0.01452\tvalidation_1-logloss:0.24048\n",
      "[241]\tvalidation_0-logloss:0.01450\tvalidation_1-logloss:0.24127\n",
      "[242]\tvalidation_0-logloss:0.01448\tvalidation_1-logloss:0.24091\n",
      "[243]\tvalidation_0-logloss:0.01446\tvalidation_1-logloss:0.24077\n",
      "[244]\tvalidation_0-logloss:0.01444\tvalidation_1-logloss:0.24040\n",
      "[245]\tvalidation_0-logloss:0.01441\tvalidation_1-logloss:0.24082\n",
      "[246]\tvalidation_0-logloss:0.01439\tvalidation_1-logloss:0.24069\n",
      "[247]\tvalidation_0-logloss:0.01437\tvalidation_1-logloss:0.24110\n",
      "[248]\tvalidation_0-logloss:0.01435\tvalidation_1-logloss:0.24124\n",
      "[249]\tvalidation_0-logloss:0.01433\tvalidation_1-logloss:0.24095\n",
      "[250]\tvalidation_0-logloss:0.01431\tvalidation_1-logloss:0.24109\n",
      "[251]\tvalidation_0-logloss:0.01428\tvalidation_1-logloss:0.24071\n",
      "[252]\tvalidation_0-logloss:0.01426\tvalidation_1-logloss:0.24058\n",
      "[253]\tvalidation_0-logloss:0.01424\tvalidation_1-logloss:0.24134\n",
      "[254]\tvalidation_0-logloss:0.01422\tvalidation_1-logloss:0.24098\n",
      "[255]\tvalidation_0-logloss:0.01420\tvalidation_1-logloss:0.24105\n",
      "[256]\tvalidation_0-logloss:0.01418\tvalidation_1-logloss:0.24093\n",
      "[257]\tvalidation_0-logloss:0.01416\tvalidation_1-logloss:0.24108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[258]\tvalidation_0-logloss:0.01414\tvalidation_1-logloss:0.24072\n",
      "[259]\tvalidation_0-logloss:0.01412\tvalidation_1-logloss:0.24087\n",
      "[260]\tvalidation_0-logloss:0.01410\tvalidation_1-logloss:0.24161\n",
      "[261]\tvalidation_0-logloss:0.01408\tvalidation_1-logloss:0.24126\n",
      "[262]\tvalidation_0-logloss:0.01406\tvalidation_1-logloss:0.24114\n",
      "[263]\tvalidation_0-logloss:0.01405\tvalidation_1-logloss:0.24131\n",
      "[264]\tvalidation_0-logloss:0.01403\tvalidation_1-logloss:0.24173\n",
      "[265]\tvalidation_0-logloss:0.01401\tvalidation_1-logloss:0.24146\n",
      "[266]\tvalidation_0-logloss:0.01399\tvalidation_1-logloss:0.24111\n",
      "[267]\tvalidation_0-logloss:0.01397\tvalidation_1-logloss:0.24100\n",
      "[268]\tvalidation_0-logloss:0.01395\tvalidation_1-logloss:0.24141\n",
      "[269]\tvalidation_0-logloss:0.01393\tvalidation_1-logloss:0.24155\n",
      "[270]\tvalidation_0-logloss:0.01392\tvalidation_1-logloss:0.24122\n",
      "[271]\tvalidation_0-logloss:0.01390\tvalidation_1-logloss:0.24136\n",
      "[272]\tvalidation_0-logloss:0.01388\tvalidation_1-logloss:0.24153\n",
      "[273]\tvalidation_0-logloss:0.01386\tvalidation_1-logloss:0.24226\n",
      "[274]\tvalidation_0-logloss:0.01384\tvalidation_1-logloss:0.24215\n",
      "[275]\tvalidation_0-logloss:0.01383\tvalidation_1-logloss:0.24180\n",
      "[276]\tvalidation_0-logloss:0.01381\tvalidation_1-logloss:0.24220\n",
      "[277]\tvalidation_0-logloss:0.01379\tvalidation_1-logloss:0.24193\n",
      "[278]\tvalidation_0-logloss:0.01377\tvalidation_1-logloss:0.24182\n",
      "[279]\tvalidation_0-logloss:0.01376\tvalidation_1-logloss:0.24197\n",
      "[280]\tvalidation_0-logloss:0.01374\tvalidation_1-logloss:0.24267\n",
      "[281]\tvalidation_0-logloss:0.01372\tvalidation_1-logloss:0.24233\n",
      "[282]\tvalidation_0-logloss:0.01371\tvalidation_1-logloss:0.24249\n",
      "[283]\tvalidation_0-logloss:0.01369\tvalidation_1-logloss:0.24216\n",
      "accuracy score: 0.9561\n",
      "roc_auc_score: 0.9530\n"
     ]
    }
   ],
   "source": [
    "#learnin_rate와 colsample_bytree는 소수 다섯째자리까지 사용\n",
    "xgb_wrapper = XGBClassifier(n_estimators=400,\n",
    "                            learning_rate = 0.08671,\n",
    "                            max_depth = 5,\n",
    "                            min_child_weight = 2,\n",
    "                            colsample_bytree = 0.62139\n",
    "                           )\n",
    "\n",
    "evals = [(X_tr, y_tr), (X_val, y_val)]\n",
    "xgb_wrapper.fit(X_tr, y_tr, early_stopping_rounds=50, eval_metric='logloss',\n",
    "                eval_set=evals, verbose=True)\n",
    "\n",
    "preds = xgb_wrapper.predict(X_test)\n",
    "pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# accuracy score과 roc auc score을 소수 넷째자리까지 출력\n",
    "print('accuracy score: {0:.4f}'.format(accuracy_score(preds, y_test)))\n",
    "print('roc_auc_score: {0:.4f}'.format(roc_auc_score(preds, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제 2. 스태킹 앙상블(25점)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드를 실행해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cancer_data = load_breast_cancer()\n",
    "\n",
    "X_data = cancer_data.data\n",
    "y_label = cancer_data.target\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_data,y_label,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 스태킹 앙상블에 사용될 KNN, randomforest,decisiontree,adaboost 총 4개의 모델을 생성해주세요. - 2점\n",
    "- 파라미터: KNN(n_neighbors=4), RF(n_estimators=100, random_state=42), ada(n_estimators=100)을 사용해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier(n_neighbors=4)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state = 42)\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 스태킹된 데이터를 로지스틱 회귀를 사용해 최종적으로 학습, 예측할 것입니다. 이를 위한 모델을 생성해주세요. - 1점\n",
    "- 파라미터: C=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_final = LogisticRegression(C = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드를 실행시켜주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개별 모델들을 학습.\n",
    "knn_clf.fit(X_train, y_train)\n",
    "rf_clf.fit(X_train , y_train)\n",
    "dt_clf.fit(X_train , y_train)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "# 개별 모델의 예측 데이터 세트 생성\n",
    "knn_pred = knn_clf.predict(X_test)\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "dt_pred = dt_clf.predict(X_test)\n",
    "ada_pred = ada_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) 예측 결과를 옆으로 붙여 피처값으로 만들어주세요. (Hint: 예측결과를 먼저 행형태로 붙인 후 행과 열을 바꿔주세요.) - 5점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 114)\n",
      "(114, 4)\n"
     ]
    }
   ],
   "source": [
    "pred = np.array([knn_pred, rf_pred, dt_pred, ada_pred])\n",
    "print(pred.shape)\n",
    "\n",
    "# transpose를 이용해 전치\n",
    "pred = np.transpose(pred)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) 위에서 생성한 데이터 세트를 최종 모델인 로지스틱 회귀로 학습, 예측시킨 후 정확도를 출력해주세요. - 4점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 메타 모델의 예측 정확도: 0.9737\n"
     ]
    }
   ],
   "source": [
    "lr_final.fit(pred, y_test)\n",
    "final = lr_final.predict(pred)\n",
    "\n",
    "print('최종 메타 모델의 예측 정확도: {0:.4f}'.format(accuracy_score(y_test, final)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 CV 스태킹 과정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) 개별 기반 모델에서 최종 메타 모델이 사용할 학습 및 테스트용 데이터를 생성하기 위한 함수를 생성해주세요. - 4점\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds ):\n",
    "    # 지정된 n_folds값으로 KFold 생성.\n",
    "    kf = KFold(n_splits = n_folds, shuffle = False)\n",
    "\n",
    "    #추후에 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화\n",
    "    train_fold_pred = np.zeros((X_train_n.shape[0], 1))\n",
    "    test_pred = np.zeros((X_test_n.shape[0], n_folds))\n",
    "    print(model.__class__.__name__, ' model 시작 ')\n",
    "    \n",
    "    for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train_n)):\n",
    "        #입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 셋 추출\n",
    "        print('|t 폴드세트: ', folder_counter, ' 시작 ')\n",
    "        X_tr = X_train_n[train_index]\n",
    "        y_tr = y_train_n[train_index]\n",
    "        X_te = X_train_n[valid_index]\n",
    "    \n",
    "        #폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행\n",
    "        model.fit(X_tr, y_tr)\n",
    "        \n",
    "        #폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장\n",
    "        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1)\n",
    "        \n",
    "        #입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장\n",
    "        test_pred[:, folder_counter] = model.predict(X_test_n)\n",
    "\n",
    "    # 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성\n",
    "    test_pred_mean = np.mean(test_pred, axis = 1).reshape(-1, 1)\n",
    "\n",
    "    #train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터\n",
    "    return train_fold_pred, test_pred_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) 위에서 생성한 함수를 활용해 모델별 학습/테스트 데이터를 분리해주세요. - 2점\n",
    "(만약 ValueError가 생긴다면, 위 함수에서 KFold의 shuffle 파라미터를 True로 설정해주세요.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier  model 시작 \n",
      "|t 폴드세트:  0  시작 \n",
      "|t 폴드세트:  1  시작 \n",
      "|t 폴드세트:  2  시작 \n",
      "|t 폴드세트:  3  시작 \n",
      "|t 폴드세트:  4  시작 \n",
      "|t 폴드세트:  5  시작 \n",
      "|t 폴드세트:  6  시작 \n",
      "RandomForestClassifier  model 시작 \n",
      "|t 폴드세트:  0  시작 \n",
      "|t 폴드세트:  1  시작 \n",
      "|t 폴드세트:  2  시작 \n",
      "|t 폴드세트:  3  시작 \n",
      "|t 폴드세트:  4  시작 \n",
      "|t 폴드세트:  5  시작 \n",
      "|t 폴드세트:  6  시작 \n",
      "DecisionTreeClassifier  model 시작 \n",
      "|t 폴드세트:  0  시작 \n",
      "|t 폴드세트:  1  시작 \n",
      "|t 폴드세트:  2  시작 \n",
      "|t 폴드세트:  3  시작 \n",
      "|t 폴드세트:  4  시작 \n",
      "|t 폴드세트:  5  시작 \n",
      "|t 폴드세트:  6  시작 \n",
      "AdaBoostClassifier  model 시작 \n",
      "|t 폴드세트:  0  시작 \n",
      "|t 폴드세트:  1  시작 \n",
      "|t 폴드세트:  2  시작 \n",
      "|t 폴드세트:  3  시작 \n",
      "|t 폴드세트:  4  시작 \n",
      "|t 폴드세트:  5  시작 \n",
      "|t 폴드세트:  6  시작 \n"
     ]
    }
   ],
   "source": [
    "knn_train, knn_test = get_stacking_base_datasets(knn_clf, X_train, y_train, X_test, 7)\n",
    "rf_train, rf_test = get_stacking_base_datasets(rf_clf, X_train, y_train, X_test, 7)\n",
    "dt_train, dt_test = get_stacking_base_datasets(dt_clf, X_train, y_train, X_test, 7)\n",
    "ada_train, ada_test = get_stacking_base_datasets(ada_clf, X_train, y_train, X_test, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) np.concatenate를 활용해 train과 test 메타 데이터를 결합해주세요. - 4점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 학습 피쳐 데이터 Shape :  (455, 30) 원본 테스트 피쳐 Shape :  (114, 30)\n",
      "스태킹 학습 피쳐 데이터 Shape: (455, 4) 스태킹 테스트 피쳐 데이터 Shape: (114, 4)\n"
     ]
    }
   ],
   "source": [
    "Stack_final_X_train = np.concatenate((knn_train, rf_train, dt_train, ada_train), axis = 1)\n",
    "Stack_final_X_test = np.concatenate((knn_test, rf_test, dt_test, ada_test), axis = 1)\n",
    "print('원본 학습 피쳐 데이터 Shape : ', X_train.shape, '원본 테스트 피쳐 Shape : ', X_test.shape)\n",
    "print('스태킹 학습 피쳐 데이터 Shape:', Stack_final_X_train.shape,\n",
    "      '스태킹 테스트 피쳐 데이터 Shape:', Stack_final_X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) 최종 메타 모델을 학습시킨 뒤, 예측 정확도를 구해주세요. - 3점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 메타 모델의 예측 정확도: 0.9649\n"
     ]
    }
   ],
   "source": [
    "lr_final.fit(Stack_final_X_train, y_train)\n",
    "stack_final = lr_final.predict(Stack_final_X_test)\n",
    "print('최종 메타 모델의 예측 정확도: {0:.4f}'.format(accuracy_score(y_test, stack_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
